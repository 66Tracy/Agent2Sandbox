"""
Agent Task Tests

Tests representative agent tasks with real LLM API and sandbox interaction.
"""

import asyncio
import sys
from pathlib import Path

# Add parent directory to path to import agent2sandbox
sys.path.insert(0, str(Path(__file__).parent.parent))

from agent2sandbox.config import Config
from agent2sandbox.llm import OpenAIClient
from agent2sandbox import (
    AgentOrchestrator,
    SandboxConfig,
)


async def test_data_analysis_task():
    """
    Task 1: Data Analysis
    Agent analyzes data, calculates statistics, and saves results to a file.

    Task:
    "è¯·åˆ†æä»¥ä¸‹æ•°æ®ï¼š[10, 20, 30, 40, 50, 60, 70, 80, 90, 100]
    è®¡ç®—å¹³å‡å€¼ã€æœ€å¤§å€¼ã€æœ€å°å€¼ã€æ ‡å‡†å·®ï¼Œå¹¶å°†ç»“æœä¿å­˜åˆ° /tmp/analysis.txt"
    """
    print("=" * 60)
    print("Task 1: Data Analysis")
    print("=" * 60)

    # Load config
    config = Config.from_env()
    config.validate()

    print(f"\n[1] Configuration:")
    print(f"   Model: {config.model_name}")
    print(f"   Sandbox: {config.sandbox_image}")

    # Create orchestrator
    sandbox_config = SandboxConfig(
        image=config.sandbox_image,
        entrypoint=["/opt/opensandbox/code-interpreter.sh"],
    )

    llm_client = OpenAIClient.from_config(config)
    orchestrator = AgentOrchestrator(sandbox_config, llm_client=llm_client)

    # Step callback for progress tracking
    def on_step(step: int, response):
        tool_calls = response.tool_calls or []
        print(f"\n[Step {step}]")
        print(f"   Response: {response.content[:80]}{'...' if len(response.content) > 80 else ''}")
        print(f"   Tool calls: {len(tool_calls)}")
        if tool_calls:
            for tc in tool_calls:
                print(f"      - {tc.name.value}")

    try:
        # Initialize orchestrator
        print("\n[2] Initializing orchestrator...")
        await orchestrator.initialize()
        print(f"   Sandbox ID: {orchestrator.state_manager.sandbox_id}")

        # Define the task
        task = """
è¯·åˆ†æä»¥ä¸‹æ•°æ®ï¼š[10, 20, 30, 40, 50, 60, 70, 80, 90, 100]

è¦æ±‚ï¼š
1. ä½¿ç”¨ Python è®¡ç®—ä»¥ä¸‹ç»Ÿè®¡é‡ï¼š
   - å¹³å‡å€¼ (mean)
   - æœ€å¤§å€¼ (max)
   - æœ€å°å€¼ (min)
   - æ ‡å‡†å·® (standard deviation)

2. å°†åˆ†æç»“æœä¿å­˜åˆ°æ–‡ä»¶ /tmp/analysis.txt

3. è¯»å–æ–‡ä»¶å†…å®¹éªŒè¯ä¿å­˜æ˜¯å¦æ­£ç¡®

4. æœ€åæ€»ç»“åˆ†æç»“æœ
"""

        print("\n[3] Starting task execution...")
        print(f"   Task: {task[:100]}...")

        # Run the task with step callback
        response = await orchestrator.run(
            task,
            max_steps=config.max_steps,
            on_step=on_step,
        )

        print("\n" + "=" * 60)
        print("Task Completion Summary")
        print("=" * 60)
        print(f"\nFinal Response:")
        print(f"   {response.content}")
        print(f"\nTotal Steps: {orchestrator.state_manager.get_step_count()}")

        # Verify task completion
        success = True
        checks = []

        # Check 1: Verify the response contains analysis
        if any(word in response.content.lower() for word in ['å¹³å‡', 'mean', 'æœ€å¤§', 'max', 'æœ€å°', 'min']):
            checks.append(("Analysis in response", True))
        else:
            checks.append(("Analysis in response", False))
            success = False

        # Check 2: Check if file was created by reading it
        try:
            file_read_result = await orchestrator.execute_tool(
                {
                    "name": "read_file",
                    "arguments": {"path": "/tmp/analysis.txt"}
                }
            )
            if file_read_result.status == "success" and file_read_result.data:
                checks.append(("File created and readable", True))
                print(f"\nFile Content:\n{file_read_result.data}")
            else:
                checks.append(("File created and readable", False))
                success = False
        except Exception as e:
            checks.append(("File created and readable", False))
            print(f"\nError reading file: {e}")
            success = False

        # Check 3: Verify statistics are in the file
        if 'mean' in file_read_result.data.lower() or 'å¹³å‡' in file_read_result.data:
            checks.append(("Statistics in file", True))
        else:
            checks.append(("Statistics in file", False))
            success = False

        # Print verification results
        print("\n" + "=" * 60)
        print("Verification Results")
        print("=" * 60)
        for check, passed in checks:
            status = "âœ…" if passed else "âŒ"
            print(f"{status} {check}")

        if success:
            print("\nğŸ‰ Data Analysis Task PASSED")
            return True
        else:
            print("\nâŒ Data Analysis Task FAILED")
            return False

    finally:
        await orchestrator.close()
        print("\n[4] Orchestrator cleaned up.")


async def test_code_debugging_task():
    """
    Task 2: Code Debugging
    Agent writes code, tests it, discovers errors, and fixes them.

    Task:
    "è¯·ç¼–å†™ä¸€ä¸ª Python å‡½æ•°æ¥è®¡ç®—æ–æ³¢é‚£å¥‘æ•°åˆ—çš„ç¬¬ n é¡¹ï¼Œ
    æµ‹è¯• n=10 çš„æƒ…å†µï¼Œå¦‚æœè¾“å‡ºä¸æ˜¯ 55ï¼Œè¯·è°ƒè¯•å¹¶ä¿®å¤ä»£ç ã€‚"
    """
    print("\n" + "=" * 60)
    print("Task 2: Code Debugging")
    print("=" * 60)

    # Load config
    config = Config.from_env()
    config.validate()

    print(f"\n[1] Configuration:")
    print(f"   Model: {config.model_name}")

    # Create orchestrator
    sandbox_config = SandboxConfig(
        image=config.sandbox_image,
        entrypoint=["/opt/opensandbox/code-interpreter.sh"],
    )

    llm_client = OpenAIClient.from_config(config)
    orchestrator = AgentOrchestrator(sandbox_config, llm_client=llm_client)

    # Step callback for progress tracking
    def on_step(step: int, response):
        tool_calls = response.tool_calls or []
        print(f"\n[Step {step}]")
        print(f"   Response: {response.content[:80]}{'...' if len(response.content) > 80 else ''}")
        print(f"   Tool calls: {len(tool_calls)}")
        if tool_calls:
            for tc in tool_calls:
                print(f"      - {tc.name.value}")

    try:
        # Initialize orchestrator
        print("\n[2] Initializing orchestrator...")
        await orchestrator.initialize()
        print(f"   Sandbox ID: {orchestrator.state_manager.sandbox_id}")

        # Define the task
        task = """
è¯·ç¼–å†™ä¸€ä¸ª Python å‡½æ•°æ¥è®¡ç®—æ–æ³¢é‚£å¥‘æ•°åˆ—çš„ç¬¬ n é¡¹ã€‚

è¦æ±‚ï¼š
1. å®ç°æ–æ³¢é‚£å¥‘å‡½æ•°ï¼ˆå¯ä»¥ä½¿ç”¨é€’å½’æˆ–è¿­ä»£ï¼‰
2. æµ‹è¯• n=10 çš„æƒ…å†µ
3. å¦‚æœè¾“å‡ºä¸æ˜¯ 55ï¼Œè¯·è°ƒè¯•ä»£ç å¹¶ä¿®å¤é”™è¯¯
4. é‡æ–°æµ‹è¯•éªŒè¯ä¿®å¤åçš„ç»“æœ
5. æ€»ç»“ä¿®å¤è¿‡ç¨‹å’Œæœ€ç»ˆç»“æœ

æ³¨æ„ï¼šæ­£ç¡®çš„æ–æ³¢é‚£å¥‘æ•°åˆ—ç¬¬10é¡¹åº”è¯¥æ˜¯ 55
"""

        print("\n[3] Starting task execution...")
        print(f"   Task: {task[:100]}...")

        # Run the task with step callback
        response = await orchestrator.run(
            task,
            max_steps=15,  # Allow more steps for debugging
            on_step=on_step,
        )

        print("\n" + "=" * 60)
        print("Task Completion Summary")
        print("=" * 60)
        print(f"\nFinal Response:")
        print(f"   {response.content}")
        print(f"\nTotal Steps: {orchestrator.state_manager.get_step_count()}")

        # Verify task completion
        success = True
        checks = []

        # Check 1: Verify the response mentions testing and debugging
        if any(word in response.content.lower() for word in ['test', 'debug', 'debugging', 'fix', 'ä¿®å¤', 'æµ‹è¯•']):
            checks.append(("Testing/debugging mentioned", True))
        else:
            checks.append(("Testing/debugging mentioned", False))
            success = False

        # Check 2: Verify the final result is 55
        if '55' in response.content:
            checks.append(("Correct result (55) in response", True))
        else:
            checks.append(("Correct result (55) in response", False))
            success = False

        # Check 3: Verify fibonacci is mentioned
        if 'fibonacci' in response.content.lower() or 'æ–æ³¢é‚£å¥‘' in response.content:
            checks.append(("Fibonacci mentioned", True))
        else:
            checks.append(("Fibonacci mentioned", False))
            success = False

        # Check 4: Verify the task involved code execution
        if orchestrator.state_manager.get_step_count() > 3:
            checks.append(("Multiple steps (reasonable debugging)", True))
        else:
            checks.append(("Multiple steps (reasonable debugging)", False))

        # Print verification results
        print("\n" + "=" * 60)
        print("Verification Results")
        print("=" * 60)
        for check, passed in checks:
            status = "âœ…" if passed else "âŒ"
            print(f"{status} {check}")

        if success:
            print("\nğŸ‰ Code Debugging Task PASSED")
            return True
        else:
            print("\nâŒ Code Debugging Task FAILED")
            return False

    finally:
        await orchestrator.close()
        print("\n[4] Orchestrator cleaned up.")


async def main():
    """Run all agent task tests."""
    print("=" * 60)
    print("Agent2Sandbox - Agent Task Tests")
    print("=" * 60)
    print("\nThese tests verify that the agent can:")
    print("  1. Analyze data and save results")
    print("  2. Write, test, and debug code")
    print("=" * 60)

    try:
        # Task 1: Data Analysis
        result1 = await test_data_analysis_task()

        # Task 2: Code Debugging
        result2 = await test_code_debugging_task()

        # Summary
        print("\n" + "=" * 60)
        print("Task Summary")
        print("=" * 60)

        results = [
            ("Data Analysis Task", result1),
            ("Code Debugging Task", result2),
        ]

        passed = sum(1 for _, result in results if result)
        total = len(results)

        for name, result in results:
            status = "âœ… PASSED" if result else "âŒ FAILED"
            print(f"{name:.<40} {status}")

        print(f"\nTotal: {passed}/{total} tasks passed")

        if passed == total:
            print("\nğŸ‰ All tasks passed!")
            print("\nâœ… Agent can successfully interact with sandbox environment")
            print("âœ… Agent can complete complex multi-step tasks")
            print("âœ… Agent can debug and fix code errors")
            return 0
        else:
            print(f"\nâš ï¸  {total - passed} task(s) failed")
            return 1

    except Exception as e:
        print(f"\nâŒ Task execution failed with error: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code)
